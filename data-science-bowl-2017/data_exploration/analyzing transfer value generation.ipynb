{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "# Fixes \"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\"\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import scipy.misc\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from sklearn import model_selection, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/kaggle_2/luna/luna16/data/pre_processed_chunks_augmented_v2_nz_single/'\n",
    "# TENSORBOARD_SUMMARIES = '/kaggle_2/luna/luna16/data/tensorboard_summaries/'\n",
    "# MODELS = '/kaggle_2/luna/luna16/models/'\n",
    "\n",
    "MODEL_PATH = '/kaggle_2/luna/luna16/models/4ba9ca74-7994-42bf-9d9f-3a8dd682e623/'\n",
    "\n",
    "#globals initializing\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "## Prediction problem specific\n",
    "tf.app.flags.DEFINE_integer('iteration_analysis', 1000,\n",
    "                            \"\"\"Number of steps after which analysis code is executed\"\"\")\n",
    "tf.app.flags.DEFINE_integer('chunk_size', 48,\n",
    "                            \"\"\"Size of chunks used.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_classes', 2,\n",
    "                            \"\"\"Number of classes to predict.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_classes_pre_process', 4,\n",
    "                            \"\"\"Number of classes before pre processing.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('batch_size', 128,\n",
    "                            \"\"\"Number of items in a batch.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_iterations', 60000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_float('reg_constant', 0.1, 'Regularization constant.')\n",
    "tf.app.flags.DEFINE_float('dropout', 0.5, 'Dropout')\n",
    "tf.app.flags.DEFINE_float('require_improvement_percentage', 0.20,\n",
    "                            \"\"\"Percent of max_iterations after which optimization will be halted if no improvement found\"\"\")\n",
    "tf.app.flags.DEFINE_float('iteration_analysis_percentage', 0.10,\n",
    "                            \"\"\"Percent of max_iterations after which analysis will be done\"\"\")\n",
    "\n",
    "## Tensorflow specific\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('allow_soft_placement', True,\n",
    "                            \"\"\"Whether to allow soft placement of calculations by tf.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('allow_growth', True,\n",
    "                            \"\"\"Whether to allow GPU growth by tf.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def img_to_rgb(im):\n",
    "    x, y, z = im.shape\n",
    "    ret = np.empty((x, y, z, 1), dtype=np.float32)\n",
    "    ret[:, :, :, 0] = im\n",
    "    return ret\n",
    "\n",
    "def get_batch(x, y):\n",
    "    num_images = len(x)\n",
    "    idx = np.random.choice(num_images,\n",
    "                           size=FLAGS.batch_size,\n",
    "                           replace=False)\n",
    "    x_batch_ids = x[idx]\n",
    "    y_batch = y[idx]\n",
    "\n",
    "    x_batch = np.ndarray([FLAGS.batch_size, FLAGS.chunk_size, FLAGS.chunk_size, FLAGS.chunk_size, 1], dtype=np.float32)\n",
    "\n",
    "    count = 0\n",
    "    for chunk_id in x_batch_ids:\n",
    "        chunk = np.load(DATA_PATH + chunk_id + '_X.npy').astype(np.float32, copy=False)\n",
    "        x_batch[count, :, :, :, :] = img_to_rgb(chunk)\n",
    "        count = count + 1\n",
    "\n",
    "    return x_batch, y_batch, idx\n",
    "\n",
    "def get_ids(PATH):\n",
    "    ids = []\n",
    "    for path in glob.glob(PATH + '*_X.npy'):\n",
    "        chunk_id = re.match(r'([0-9a-f-]+)_X.npy', os.path.basename(path)).group(1)\n",
    "        ids.append(chunk_id)\n",
    "    return ids\n",
    "\n",
    "def get_data(chunk_ids, PATH):\n",
    "    X = np.asarray(chunk_ids)\n",
    "    Y = np.ndarray([len(chunk_ids), FLAGS.num_classes_pre_process], dtype=np.float32)\n",
    "\n",
    "    count = 0\n",
    "    for chunk_id in chunk_ids:\n",
    "        y = np.load(PATH + chunk_id + '_Y.npy').astype(np.float32, copy=False)\n",
    "        Y[count, :] = y\n",
    "        count = count + 1\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to load data: 0:00:18\n",
      "Splitting into train, validation sets\n",
      "Total time to split: 0:00:18\n",
      "train_x: (55936,)\n",
      "validation_x: (13984,)\n",
      "train_y: (55936, 2)\n",
      "validation_y: (13984, 2)\n"
     ]
    }
   ],
   "source": [
    "time0 = time.time()\n",
    "chunks_ids = get_ids(DATA_PATH)\n",
    "X, Y = get_data(chunks_ids, DATA_PATH)\n",
    "\n",
    "print(\"Total time to load data: \" + str(timedelta(seconds=int(round(time.time() - time0)))))\n",
    "print('Splitting into train, validation sets')\n",
    "Y = np.argmax(Y, axis = 1)\n",
    "\n",
    "# Crunch 4 classes to 2\n",
    "Y[Y == 2] = 1\n",
    "Y[Y == 3] = 1\n",
    "\n",
    "train_x, validation_x, train_y, validation_y = model_selection.train_test_split(X, Y, random_state=42, stratify=Y, test_size=0.20)\n",
    "\n",
    "del X\n",
    "del Y\n",
    "print(\"Total time to split: \" + str(timedelta(seconds=int(round(time.time() - time0)))))\n",
    "\n",
    "train_y = (np.arange(FLAGS.num_classes) == train_y[:, None])+0\n",
    "validation_y = (np.arange(FLAGS.num_classes) == validation_y[:, None])+0\n",
    "\n",
    "print('train_x: {}'.format(train_x.shape))\n",
    "print('validation_x: {}'.format(validation_x.shape))\n",
    "print('train_y: {}'.format(train_y.shape))\n",
    "print('validation_y: {}'.format(validation_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DATA_PATH = '/kaggle_3/stage1_processed_unseg/'\n",
    "# DATA_PATH2 = '/kaggle_2/stage2_processed_unseg/'\n",
    "# OUTPUT_PATH = '/kaggle_3/all_stage_features/'\n",
    "PATIENT_SCANS = 'scan_lungs_'\n",
    "TENSORBOARD_SUMMARIES = '/kaggle/dev/data-science-bowl-2017-data/tensorboard_summaries_checking/'\n",
    "MODEL_PATH = '/kaggle_2/luna/luna16/models/4ba9ca74-7994-42bf-9d9f-3a8dd682e623/'\n",
    "OVERLAP_PERCENTAGE = 0.7\n",
    "klass_weights = np.asarray([69838.0/40513.0, 69838.0/29325.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv3d(inputs,             # The previous layer.\n",
    "           filter_size,        # Width and height of each filter.\n",
    "           num_filters,        # Number of filters.\n",
    "           num_channels,       # 1\n",
    "           strides,            # [1,1,1,1,1]\n",
    "           layer_name):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            filters = tf.get_variable(layer_name + 'weights', shape = [filter_size, filter_size, filter_size, num_channels, num_filters],\n",
    "                                      initializer = tf.truncated_normal_initializer(stddev=1e-1, dtype=tf.float32),\n",
    "                                      regularizer = tf.contrib.layers.l2_regularizer(FLAGS.reg_constant))\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[num_filters], dtype=tf.float32))\n",
    "        with tf.name_scope('conv'):\n",
    "            conv = tf.nn.conv3d(inputs, filters, strides, padding='SAME')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        return out, filters\n",
    "\n",
    "def max_pool_3d(inputs,\n",
    "                filter_size,\n",
    "                strides,\n",
    "                layer_name):\n",
    "    with tf.name_scope(layer_name):\n",
    "        return tf.nn.max_pool3d(inputs,\n",
    "                                ksize=filter_size,\n",
    "                                strides=strides,\n",
    "                                padding='SAME',\n",
    "                                name='max_pool')\n",
    "\n",
    "def dropout_3d(inputs,\n",
    "               keep_prob,\n",
    "               layer_name):\n",
    "    with tf.name_scope(layer_name):\n",
    "        return tf.nn.dropout(inputs, keep_prob, name='dropout')\n",
    "\n",
    "def flatten_3d(layer, layer_name):\n",
    "    with tf.name_scope(layer_name):\n",
    "        layer_shape = layer.get_shape()\n",
    "        num_features = layer_shape[1:5].num_elements()\n",
    "        layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "        return layer_flat, num_features\n",
    "\n",
    "def relu_3d(inputs,\n",
    "            layer_name):\n",
    "    with tf.name_scope(layer_name):\n",
    "        return tf.nn.relu(inputs, name='relu')\n",
    "\n",
    "def dense_3d(inputs,\n",
    "             num_inputs,\n",
    "             num_outputs,\n",
    "             layer_name):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = tf.get_variable(layer_name + 'weights', shape = [num_inputs, num_outputs],\n",
    "                              initializer = tf.truncated_normal_initializer(stddev=1e-1, dtype=tf.float32),\n",
    "                              regularizer = tf.contrib.layers.l2_regularizer(FLAGS.reg_constant))\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[num_outputs], dtype=tf.float32))\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            layer = tf.matmul(inputs, weights) + biases\n",
    "        return layer\n",
    "\n",
    "def worker():\n",
    "\n",
    "    def get_validation_batch(validation_x_ids, validation_y, batch_number):\n",
    "        num_images = len(validation_x_ids)\n",
    "\n",
    "        count = 0\n",
    "        start_index = batch_number * FLAGS.batch_size\n",
    "        end_index = start_index + FLAGS.batch_size\n",
    "        end_index = num_images if end_index > num_images else end_index\n",
    "        real_batch_size = end_index - start_index\n",
    "\n",
    "        validation_x = np.ndarray([real_batch_size, FLAGS.chunk_size, FLAGS.chunk_size, FLAGS.chunk_size, 1], dtype=np.float32)\n",
    "        \n",
    "        for chunk_id in validation_x_ids[start_index : end_index]:\n",
    "            chunk = np.load(DATA_PATH + chunk_id + '_X.npy').astype(np.float32, copy=False)\n",
    "            validation_x[count, :, :, :, :] = img_to_rgb(chunk)\n",
    "            count = count + 1\n",
    "\n",
    "        return validation_x, validation_y[start_index : end_index]\n",
    "    \n",
    "    def feed_dict(is_train, batch_number = 0):\n",
    "        if is_train:\n",
    "            x_batch, y_batch = get_batch(train_x, train_y)\n",
    "            k = FLAGS.dropout\n",
    "        else:\n",
    "            x_batch, y_batch = get_validation_batch(validation_x, validation_y, batch_number)\n",
    "            k = 1.0\n",
    "        crss_entrpy_weights = np.ones((y_batch.shape[0]))\n",
    "        for m in range(y_batch.shape[0]):\n",
    "            crss_entrpy_weights[m] = np.amax(y_batch[m] * klass_weights)\n",
    "        return {x: x_batch, y_labels: y_batch, keep_prob: k, cross_entropy_weights: crss_entrpy_weights}\n",
    "\n",
    "    \n",
    "    \n",
    "    # Graph construction\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        x = tf.placeholder(tf.float32, shape=[None, FLAGS.chunk_size, FLAGS.chunk_size, FLAGS.chunk_size, 1], name = 'x')\n",
    "        y = tf.placeholder(tf.float32, shape=[None, FLAGS.num_classes], name = 'y')\n",
    "        y_labels = tf.placeholder(tf.float32, shape=[None, FLAGS.num_classes], name ='y_labels')\n",
    "        cross_entropy_weights = tf.placeholder(tf.float32, shape=[None], name ='cross_entropy_weights')\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        class_weights_base = tf.ones_like(y_labels)\n",
    "        class_weights = tf.multiply(class_weights_base , [69838.0/40513.0, 69838.0/29325.0])\n",
    "\n",
    "        # layer1\n",
    "        conv1_1_out, conv1_1_weights = conv3d(inputs = x, filter_size = 3, num_filters = 16, num_channels = 1, strides = [1, 3, 3, 3, 1], layer_name ='conv1_1')\n",
    "        relu1_1_out = relu_3d(inputs = conv1_1_out, layer_name='relu1_1')\n",
    "\n",
    "        conv1_2_out, conv1_2_weights = conv3d(inputs = relu1_1_out, filter_size = 3, num_filters = 16, num_channels = 16, strides = [1, 3, 3, 3, 1], layer_name ='conv1_2')\n",
    "        relu1_2_out = relu_3d(inputs = conv1_2_out, layer_name='relu1_2')\n",
    "\n",
    "        pool1_out = max_pool_3d(inputs = relu1_2_out, filter_size = [1, 2, 2, 2, 1], strides = [1, 2, 2, 2, 1], layer_name ='pool1')\n",
    "\n",
    "        # layer2\n",
    "        conv2_1_out, conv2_1_weights = conv3d(inputs = pool1_out, filter_size = 3, num_filters = 32, num_channels = 16, strides = [1, 3, 3, 3, 1], layer_name ='conv2_1')\n",
    "        relu2_1_out = relu_3d(inputs = conv2_1_out, layer_name='relu2_1')\n",
    "\n",
    "        conv2_2_out, conv2_2_weights = conv3d(inputs = relu2_1_out, filter_size = 3, num_filters = 32, num_channels = 32, strides = [1, 3, 3, 3, 1], layer_name ='conv2_2')\n",
    "        relu2_2_out = relu_3d(inputs = conv2_2_out, layer_name='relu2_2')\n",
    "\n",
    "        pool2_out = max_pool_3d(inputs = relu2_2_out, filter_size = [1, 2, 2, 2, 1], strides = [1, 2, 2, 2, 1], layer_name ='pool2')\n",
    "\n",
    "        # layer3\n",
    "        conv3_1_out, conv3_1_weights = conv3d(inputs = pool2_out, filter_size = 3, num_filters = 64, num_channels = 32, strides = [1, 3, 3, 3, 1], layer_name ='conv3_1')\n",
    "        relu3_1_out = relu_3d(inputs = conv3_1_out, layer_name='relu3_1')\n",
    "\n",
    "        conv3_2_out, conv3_2_weights = conv3d(inputs = relu3_1_out, filter_size = 3, num_filters = 64, num_channels = 64, strides = [1, 3, 3, 3, 1], layer_name ='conv3_2')\n",
    "        relu3_2_out = relu_3d(inputs = conv3_2_out, layer_name='relu3_2')\n",
    "\n",
    "        conv3_3_out, conv3_3_weights = conv3d(inputs = relu3_2_out, filter_size = 3, num_filters = 64, num_channels = 64, strides = [1, 3, 3, 3, 1], layer_name ='conv3_3')\n",
    "        relu3_3_out = relu_3d(inputs = conv3_3_out, layer_name='relu3_3')\n",
    "\n",
    "        pool3_out = max_pool_3d(inputs = relu3_3_out, filter_size = [1, 2, 2, 2, 1], strides = [1, 2, 2, 2, 1], layer_name ='pool3')\n",
    "\n",
    "        # layer4\n",
    "        conv4_1_out, conv4_1_weights = conv3d(inputs = pool3_out, filter_size = 3, num_filters = 128, num_channels = 64, strides = [1, 3, 3, 3, 1], layer_name ='conv4_1')\n",
    "        relu4_1_out = relu_3d(inputs = conv4_1_out, layer_name='relu4_1')\n",
    "\n",
    "        conv4_2_out, conv4_2_weights = conv3d(inputs = relu4_1_out, filter_size = 3, num_filters = 128, num_channels = 128, strides = [1, 3, 3, 3, 1], layer_name ='conv4_2')\n",
    "        relu4_2_out = relu_3d(inputs = conv4_2_out, layer_name='relu4_2')\n",
    "\n",
    "        conv4_3_out, conv4_3_weights = conv3d(inputs = relu4_2_out, filter_size = 3, num_filters = 128, num_channels = 128, strides = [1, 3, 3, 3, 1], layer_name ='conv4_3')\n",
    "        relu4_3_out = relu_3d(inputs = conv4_3_out, layer_name='relu4_3')\n",
    "\n",
    "        pool4_out = max_pool_3d(inputs = relu4_3_out, filter_size = [1, 2, 2, 2, 1], strides = [1, 2, 2, 2, 1], layer_name ='pool4')\n",
    "\n",
    "        # layer5\n",
    "        conv5_1_out, conv5_1_weights = conv3d(inputs = pool4_out, filter_size = 3, num_filters = 256, num_channels = 128, strides = [1, 3, 3, 3, 1], layer_name ='conv5_1')\n",
    "        relu5_1_out = relu_3d(inputs = conv5_1_out, layer_name='relu5_1')\n",
    "\n",
    "        conv5_2_out, conv5_2_weights = conv3d(inputs = relu5_1_out, filter_size = 3, num_filters = 256, num_channels = 256, strides = [1, 3, 3, 3, 1], layer_name ='conv5_2')\n",
    "        relu5_2_out = relu_3d(inputs = conv5_2_out, layer_name='relu5_2')\n",
    "\n",
    "        conv5_3_out, conv5_3_weights = conv3d(inputs = relu5_2_out, filter_size = 3, num_filters = 256, num_channels = 256, strides = [1, 3, 3, 3, 1], layer_name ='conv5_3')\n",
    "        relu5_3_out = relu_3d(inputs = conv5_3_out, layer_name='relu5_3')\n",
    "\n",
    "        pool5_out = max_pool_3d(inputs = relu5_3_out, filter_size = [1, 2, 2, 2, 1], strides = [1, 2, 2, 2, 1], layer_name ='pool5')\n",
    "        flatten5_out, flatten5_features = flatten_3d(pool5_out, layer_name='flatten5')\n",
    "\n",
    "        # layer6\n",
    "        dense6_out = dense_3d(inputs=flatten5_out, num_inputs=int(flatten5_out.shape[1]), num_outputs=4096, layer_name ='fc6')\n",
    "        relu6_out = relu_3d(inputs = dense6_out, layer_name='relu6')\n",
    "        dropout6_out = dropout_3d(inputs = relu6_out, keep_prob = 0.5, layer_name='drop6')\n",
    "\n",
    "        # layer7\n",
    "        dense7_out = dense_3d(inputs=dropout6_out, num_inputs=int(dropout6_out.shape[1]), num_outputs=4096, layer_name ='fc7')\n",
    "        relu7_out = relu_3d(inputs = dense7_out, layer_name='relu7')\n",
    "        dropout7_out = dropout_3d(inputs = relu7_out, keep_prob = 0.5, layer_name='drop7')\n",
    "\n",
    "        # layer8\n",
    "        dense8_out = dense_3d(inputs=dropout7_out, num_inputs=int(dropout7_out.shape[1]), num_outputs=1000, layer_name ='fc8')\n",
    "\n",
    "        # layer9\n",
    "        dense9_out = dense_3d(inputs=dense8_out, num_inputs=int(dense8_out.shape[1]), num_outputs=FLAGS.num_classes, layer_name ='fc9')\n",
    "\n",
    "        # Final softmax\n",
    "        y = tf.nn.softmax(dense9_out)\n",
    "\n",
    "        # Overall Metrics Calculations\n",
    "        with tf.name_scope('log_loss'):\n",
    "            log_loss = tf.losses.log_loss(y_labels, y, epsilon=10e-15)\n",
    "            tf.summary.scalar('log_loss', log_loss)\n",
    "\n",
    "        with tf.name_scope('softmax_cross_entropy'):\n",
    "            softmax_cross_entropy = tf.losses.softmax_cross_entropy(y_labels, dense9_out)\n",
    "            tf.summary.scalar('softmax_cross_entropy', softmax_cross_entropy)\n",
    "\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_labels, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "        with tf.name_scope('weighted_log_loss'):\n",
    "            weighted_log_loss = tf.losses.log_loss(y_labels, y, weights=class_weights, epsilon=10e-15) + tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "            tf.summary.scalar('weighted_log_loss', weighted_log_loss)\n",
    "\n",
    "        with tf.name_scope('weighted_softmax_cross_entropy'):\n",
    "            weighted_softmax_cross_entropy = tf.losses.softmax_cross_entropy(y_labels, dense9_out, weights=cross_entropy_weights)\n",
    "            tf.summary.scalar('weighted_softmax_cross_entropy', weighted_softmax_cross_entropy)\n",
    "\n",
    "        with tf.name_scope('sparse_softmax_cross_entropy'):\n",
    "            y_labels_argmax_int = tf.to_int32(tf.argmax(y_labels, axis=1))\n",
    "            sparse_softmax_cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_labels_argmax_int, logits=dense9_out)\n",
    "            tf.summary.scalar('sparse_softmax_cross_entropy', sparse_softmax_cross_entropy)\n",
    "\n",
    "        with tf.name_scope('weighted_sparse_softmax_cross_entropy'):\n",
    "            y_labels_argmax_int = tf.to_int32(tf.argmax(y_labels, axis=1))\n",
    "            weighted_sparse_softmax_cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_labels_argmax_int, logits=dense9_out, weights=cross_entropy_weights)\n",
    "            tf.summary.scalar('weighted_sparse_softmax_cross_entropy', weighted_sparse_softmax_cross_entropy)\n",
    "\n",
    "        # Class Based Metrics calculations\n",
    "        y_pred_class = tf.argmax(y, 1)\n",
    "        y_labels_class = tf.argmax(y_labels, 1)\n",
    "\n",
    "        confusion_matrix = tf.confusion_matrix(y_labels_class, y_pred_class, num_classes=FLAGS.num_classes)\n",
    "\n",
    "        sum_row_0 = tf.reduce_sum(confusion_matrix[0, :])\n",
    "        sum_row_1 = tf.reduce_sum(confusion_matrix[1, :])\n",
    "        # sum_row_2 = tf.reduce_sum(confusion_matrix[2, :])\n",
    "        # sum_row_3 = tf.reduce_sum(confusion_matrix[3, :])\n",
    "        sum_col_0 = tf.reduce_sum(confusion_matrix[:, 0])\n",
    "        sum_col_1 = tf.reduce_sum(confusion_matrix[:, 1])\n",
    "        # sum_col_2 = tf.reduce_sum(confusion_matrix[:, 2])\n",
    "        # sum_col_3 = tf.reduce_sum(confusion_matrix[:, 3])\n",
    "\n",
    "        sum_all = tf.reduce_sum(confusion_matrix[:, :])\n",
    "\n",
    "        with tf.name_scope('precision'):\n",
    "            precision_0 = confusion_matrix[0,0] / sum_col_0\n",
    "            precision_1 = confusion_matrix[1,1] / sum_col_1\n",
    "            # precision_2 = confusion_matrix[2,2] / sum_col_2\n",
    "            # precision_3 = confusion_matrix[3,3] / sum_col_3\n",
    "\n",
    "            tf.summary.scalar('precision_0', precision_0)\n",
    "            tf.summary.scalar('precision_1', precision_1)\n",
    "            # tf.summary.scalar('precision_2', precision_2)\n",
    "            # tf.summary.scalar('precision_3', precision_3)\n",
    "\n",
    "        with tf.name_scope('recall'):\n",
    "            recall_0 = confusion_matrix[0,0] / sum_row_0\n",
    "            recall_1 = confusion_matrix[1,1] / sum_row_1\n",
    "            # recall_2 = confusion_matrix[2,2] / sum_row_2\n",
    "            # recall_3 = confusion_matrix[3,3] / sum_row_3\n",
    "\n",
    "            tf.summary.scalar('recall_0', recall_0)\n",
    "            tf.summary.scalar('recall_1', recall_1)\n",
    "            # tf.summary.scalar('recall_2', recall_2)\n",
    "            # tf.summary.scalar('recall_3', recall_3)\n",
    "\n",
    "        with tf.name_scope('specificity'):\n",
    "            tn_0 = sum_all - (sum_row_0 + sum_col_0 - confusion_matrix[0,0])\n",
    "            fp_0 = sum_col_0 - confusion_matrix[0,0]\n",
    "            specificity_0 = tn_0 / (tn_0 + fp_0)\n",
    "\n",
    "            tn_1 = sum_all - (sum_row_1 + sum_col_1 - confusion_matrix[1,1])\n",
    "            fp_1 = sum_col_1 - confusion_matrix[1,1]\n",
    "            specificity_1 = tn_1 / (tn_1 + fp_1)\n",
    "\n",
    "            # tn_2 = sum_all - (sum_row_2 + sum_col_2 - confusion_matrix[2,2])\n",
    "            # fp_2 = sum_col_2 - confusion_matrix[2,2]\n",
    "            # specificity_2 = tn_2 / (tn_2 + fp_2)\n",
    "            #\n",
    "            # tn_3 = sum_all - (sum_row_3 + sum_col_3 - confusion_matrix[3,3])\n",
    "            # fp_3 = sum_col_3 - confusion_matrix[3,3]\n",
    "            # specificity_3 = tn_3 / (tn_3 + fp_3)\n",
    "\n",
    "            tf.summary.scalar('specificity_0', specificity_0)\n",
    "            tf.summary.scalar('specificity_1', specificity_1)\n",
    "            # tf.summary.scalar('specificity_2', specificity_2)\n",
    "            # tf.summary.scalar('specificity_3', specificity_3)\n",
    "\n",
    "        with tf.name_scope('true_positives'):\n",
    "            tp_0 = confusion_matrix[0,0]\n",
    "            tp_1 = confusion_matrix[1,1]\n",
    "            # tp_2 = confusion_matrix[2,2]\n",
    "            # tp_3 = confusion_matrix[3,3]\n",
    "\n",
    "            tf.summary.scalar('true_positives_0', tp_0)\n",
    "            tf.summary.scalar('true_positives_1', tp_1)\n",
    "            # tf.summary.scalar('true_positives_2', tp_2)\n",
    "            # tf.summary.scalar('true_positives_3', tp_3)\n",
    "\n",
    "        with tf.name_scope('true_negatives'):\n",
    "            tf.summary.scalar('true_negatives_0', tn_0)\n",
    "            tf.summary.scalar('true_negatives_1', tn_1)\n",
    "            # tf.summary.scalar('true_negatives_2', tn_2)\n",
    "            # tf.summary.scalar('true_negatives_3', tn_3)\n",
    "\n",
    "        with tf.name_scope('false_positives'):\n",
    "            tf.summary.scalar('false_positives_0', fp_0)\n",
    "            tf.summary.scalar('false_positives_1', fp_1)\n",
    "            # tf.summary.scalar('false_positives_2', fp_2)\n",
    "            # tf.summary.scalar('false_positives_3', fp_3)\n",
    "\n",
    "        with tf.name_scope('false_negatives'):\n",
    "            fn_0 = sum_row_0 - tp_0\n",
    "            fn_1 = sum_row_1 - tp_1\n",
    "            # fn_2 = sum_row_2 - tp_2\n",
    "            # fn_3 = sum_row_3 - tp_3\n",
    "\n",
    "            tf.summary.scalar('false_negatives_0', fn_0)\n",
    "            tf.summary.scalar('false_negatives_1', fn_1)\n",
    "            # tf.summary.scalar('false_negatives_2', fn_2)\n",
    "            # tf.summary.scalar('false_negatives_3', fn_3)\n",
    "\n",
    "        with tf.name_scope('log_loss_by_class'):\n",
    "            log_loss_0 = tf.losses.log_loss(y_labels[0], y[0], epsilon=10e-15)\n",
    "            log_loss_1 = tf.losses.log_loss(y_labels[1], y[1], epsilon=10e-15)\n",
    "            # log_loss_2 = tf.losses.log_loss(y_labels[2], y[2], epsilon=10e-15)\n",
    "            # log_loss_3 = tf.losses.log_loss(y_labels[3], y[3], epsilon=10e-15)\n",
    "\n",
    "            #added extra '_' to avoid tenosorboard name collision with the main log_loss metric\n",
    "            tf.summary.scalar('log_loss__0', log_loss_0)\n",
    "            tf.summary.scalar('log_loss__1', log_loss_1)\n",
    "            # tf.summary.scalar('log_loss__2', log_loss_2)\n",
    "            # tf.summary.scalar('log_loss__3', log_loss_3)\n",
    "\n",
    "        with tf.name_scope('softmax_cross_entropy_by_class'):\n",
    "            softmax_cross_entropy_0 = tf.losses.softmax_cross_entropy(y_labels[0], dense9_out[0])\n",
    "            softmax_cross_entropy_1 = tf.losses.softmax_cross_entropy(y_labels[1], dense9_out[1])\n",
    "            # softmax_cross_entropy_2 = tf.losses.softmax_cross_entropy(y_labels[2], dense9_out[2])\n",
    "            # softmax_cross_entropy_3 = tf.losses.softmax_cross_entropy(y_labels[3], dense9_out[3])\n",
    "\n",
    "            tf.summary.scalar('softmax_cross_entropy_0', softmax_cross_entropy_0)\n",
    "            tf.summary.scalar('softmax_cross_entropy_1', softmax_cross_entropy_1)\n",
    "            # tf.summary.scalar('softmax_cross_entropy_2', softmax_cross_entropy_2)\n",
    "            # tf.summary.scalar('softmax_cross_entropy_3', softmax_cross_entropy_3)\n",
    "\n",
    "        with tf.name_scope('accuracy_by_class'):\n",
    "            accuracy_0 = (tp_0 + tn_0)/(tp_0 + fp_0 + fn_0 + tn_0)\n",
    "            accuracy_1 = (tp_1 + tn_1)/(tp_1 + fp_1 + fn_1 + tn_1)\n",
    "            # accuracy_2 = (tp_2 + tn_2)/(tp_2 + fp_2 + fn_2 + tn_2)\n",
    "            # accuracy_3 = (tp_3 + tn_3)/(tp_3 + fp_3 + fn_3 + tn_3)\n",
    "\n",
    "            tf.summary.scalar('accuracy_0', accuracy_0)\n",
    "            tf.summary.scalar('accuracy_1', accuracy_1)\n",
    "            # tf.summary.scalar('accuracy_2', accuracy_2)\n",
    "            # tf.summary.scalar('accuracy_3', accuracy_3)\n",
    "\n",
    "        with tf.name_scope('weighted_log_loss_by_class'):\n",
    "            weighted_log_loss_0 = tf.losses.log_loss(y_labels[0], y[0], weights=class_weights[0], epsilon=10e-15)\n",
    "            weighted_log_loss_1 = tf.losses.log_loss(y_labels[1], y[1], weights=class_weights[1], epsilon=10e-15)\n",
    "            # weighted_log_loss_2 = tf.losses.log_loss(y_labels[2], y[2], weights=class_weights[2], epsilon=10e-15)\n",
    "            # weighted_log_loss_3 = tf.losses.log_loss(y_labels[3], y[3], weights=class_weights[3], epsilon=10e-15)\n",
    "\n",
    "            tf.summary.scalar('weighted_log_loss_0', weighted_log_loss_0)\n",
    "            tf.summary.scalar('weighted_log_loss_1', weighted_log_loss_1)\n",
    "            # tf.summary.scalar('weighted_log_loss_2', weighted_log_loss_2)\n",
    "            # tf.summary.scalar('weighted_log_loss_3', weighted_log_loss_3)\n",
    "\n",
    "        with tf.name_scope('f1_score_by_class'):\n",
    "            f1_score_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0)\n",
    "            f1_score_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1)\n",
    "            # f1_score_2 = 2 * (precision_2 * recall_2) / (precision_2 + recall_2)\n",
    "            # f1_score_3 = 2 * (precision_3 * recall_3) / (precision_3 + recall_3)\n",
    "            # #f1_score = (f1_score_0 * 40591.0/69920.0) + (f1_score_1 * 14624.0/69920.0) + (f1_score_2 * 10490.0/69920.0) + (f1_score_3 *4215.0/ 69920.0)\n",
    "            tf.summary.scalar('f1_score_0', f1_score_0)\n",
    "            tf.summary.scalar('f1_score_1', f1_score_1)\n",
    "            # tf.summary.scalar('f1_score_2', f1_score_2)\n",
    "            # tf.summary.scalar('f1_score_3', f1_score_3)\n",
    "\n",
    "#         with tf.name_scope('train'):\n",
    "#             optimizer = tf.train.AdamOptimizer(learning_rate=1e-4, name='adam_optimizer').minimize(softmax_cross_entropy)\n",
    "\n",
    "        merged = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    \n",
    "    start_timestamp = str(int(time.time()))\n",
    "    test_run_name = 'checking-'+start_timestamp\n",
    "\n",
    "    # Setting up config\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = FLAGS.allow_growth\n",
    "    config.log_device_placement=FLAGS.log_device_placement\n",
    "    config.allow_soft_placement=FLAGS.allow_soft_placement\n",
    "    \n",
    "    k_count = 0\n",
    "    with tf.Session(graph=graph, config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.import_meta_graph(MODEL_PATH + 'model.meta')\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(MODEL_PATH))\n",
    "        test_writer = tf.summary.FileWriter(TENSORBOARD_SUMMARIES + test_run_name, sess.graph)\n",
    "\n",
    "        num_batches = int(math.ceil(float(len(validation_x)) / FLAGS.batch_size))\n",
    "        print('num_batches', num_batches)\n",
    "        for k in range(num_batches):\n",
    "            _, step_summary = sess.run([y, merged], feed_dict=feed_dict(False, k))\n",
    "            test_writer.add_summary(step_summary, k_count)\n",
    "            k_count = k_count + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 110\n"
     ]
    }
   ],
   "source": [
    "worker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
